{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'passes.csv'\n",
    "# read the csv file\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Distance(x1,x2,y1,y2):\n",
    "    import math\n",
    "    dist = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "    # return dist \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting based on first column (time)\n",
    "df = df.sort_values(by=['time_start'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = []  # Ensure df2 is an empty list\n",
    "\n",
    "# Iterate through the rows\n",
    "for i in range(len(df)):\n",
    "    # Extract row data\n",
    "    row = df.iloc[i]\n",
    "    X = int(row['sender_id'])\n",
    "    Y = int(row['receiver_id'])\n",
    "    time = row['time_start']\n",
    "    end_time = row['time_end']\n",
    "    X1_distance = row[f'x_{X}']\n",
    "    Y1_distance = row[f'y_{X}']\n",
    "    X2_distance = row[f'x_{Y}']\n",
    "    Y2_distance = row[f'y_{Y}']\n",
    "    \n",
    "    # Calculate distance and append to df2\n",
    "    temp = {\n",
    "        'time_start': time,\n",
    "        'time_end': end_time,\n",
    "        'Sender': X,\n",
    "        'Receiver': Y,\n",
    "        'Distance': Distance(X1_distance, X2_distance, Y1_distance, Y2_distance)\n",
    "    }\n",
    "    df2.append(temp)\n",
    "\n",
    "# Optionally convert df2 to a DataFrame if needed\n",
    "df2 = pd.DataFrame(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the 'time_start'and 'time_end' from ms to s\n",
    "df2['time_start'] = df2['time_start'] / 1000\n",
    "df2['time_end'] = df2['time_end'] / 1000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtered Passes Binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # Define the input and output file paths\n",
    "# input_file_path = 'passes.csv'  # Replace with your actual file path\n",
    "# output_file_path = 'filtered_passes_binary.csv'\n",
    "\n",
    "# # Ensure the file exists\n",
    "# if not os.path.exists(input_file_path):\n",
    "#     print(f\"Error: The file '{input_file_path}' does not exist.\")\n",
    "# else:\n",
    "#     try:\n",
    "#         # Read the original CSV file\n",
    "#         df = pd.read_csv(input_file_path)\n",
    "\n",
    "#         # Check if the necessary columns exist\n",
    "#         main_columns = ['time_start', 'time_end', 'sender_id', 'receiver_id']\n",
    "#         target_columns = [f'x_{i}' for i in range(1, 29)]  # x1 to x28\n",
    "#         all_columns = main_columns + target_columns\n",
    "\n",
    "#         missing_columns = [col for col in all_columns if col not in df.columns]\n",
    "#         if missing_columns:\n",
    "#             print(f\"Error: The following columns are missing: {', '.join(missing_columns)}\")\n",
    "#         else:\n",
    "#             # Function to create the binary sequence for empty and non-empty cells\n",
    "#             def create_binary_sequence(row):\n",
    "#                 return ''.join(['0' if pd.isna(row[col]) else '1' for col in target_columns])\n",
    "\n",
    "#             # Apply the function to create the binary sequence column\n",
    "#             df['empty_columns_sequence'] = df[target_columns].apply(create_binary_sequence, axis=1)\n",
    "\n",
    "#             # Select the required columns for the output\n",
    "#             filtered_df = df[main_columns + ['empty_columns_sequence']]\n",
    "\n",
    "#             # Save the filtered DataFrame to a new CSV file\n",
    "#             filtered_df.to_csv(output_file_path, index=False)\n",
    "#             print(f\"Filtered file saved successfully to {output_file_path}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sir's logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def split_csv(input_file):\n",
    "#     # Read the CSV file\n",
    "#     df = pd.read_csv(input_file)\n",
    "    \n",
    "#     # Initialize variables\n",
    "#     file_count = 0\n",
    "#     current_file_data = []\n",
    "#     last_end_time = None\n",
    "#     last_receiver_id = None\n",
    "#     last_empty_sequence = None\n",
    "\n",
    "#     for index, row in df.iterrows():\n",
    "#         start_time = row['time_start']\n",
    "#         end_time = row['time_end']\n",
    "#         sender_id = row['sender_id']\n",
    "#         receiver_id = row['receiver_id']\n",
    "#         empty_columns_sequence = row['empty_columns_sequence']\n",
    "\n",
    "#         # If it's the first row, initialize the first file\n",
    "#         if index == 0:\n",
    "#             current_file_data.append(row)\n",
    "#             last_end_time = end_time\n",
    "#             last_receiver_id = receiver_id\n",
    "#             last_empty_sequence = empty_columns_sequence\n",
    "#             continue\n",
    "\n",
    "#         # Check if the current row should be added to the current file\n",
    "#         if start_time <= last_end_time and receiver_id == last_receiver_id:\n",
    "#             current_file_data.append(row)\n",
    "#             last_end_time = end_time  # Update last end time\n",
    "#             last_receiver_id = receiver_id  # Update last receiver id\n",
    "#             last_empty_sequence = empty_columns_sequence  # Update last empty sequence\n",
    "#         else:\n",
    "#             # Check if we need to create a new file\n",
    "#             if last_empty_sequence != empty_columns_sequence:\n",
    "#                 # Save the current file data to a CSV\n",
    "#                 output_file = f'this/output_file_{file_count}.csv'\n",
    "#                 pd.DataFrame(current_file_data).to_csv(output_file, index=False)\n",
    "                \n",
    "#                 # Increment file count and reset for the new file\n",
    "#                 file_count += 1\n",
    "#                 current_file_data = [row]  # Start new file with the current row\n",
    "#                 last_end_time = end_time\n",
    "#                 last_receiver_id = receiver_id\n",
    "#                 last_empty_sequence = empty_columns_sequence\n",
    "#             else:\n",
    "#                 # If the empty column sequence matches, just append to the current file\n",
    "#                 current_file_data.append(row)\n",
    "#                 last_end_time = end_time\n",
    "#                 last_receiver_id = receiver_id\n",
    "#                 last_empty_sequence = empty_columns_sequence\n",
    "\n",
    "#     # Save any remaining data to a final file\n",
    "#     if current_file_data:\n",
    "#         output_file = f'this/output_file_{file_count}.csv'\n",
    "#         pd.DataFrame(current_file_data).to_csv(output_file, index=False)\n",
    "\n",
    "# # Example usage\n",
    "# split_csv('filtered_passes_binary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # Define the path to your CSV file\n",
    "# input_file_path = 'passes.csv'  # Replace with your actual file path\n",
    "\n",
    "# # Define the base folder to save match files\n",
    "# base_output_folder = 'matches'\n",
    "\n",
    "# # Create the base output folder if it doesn't exist\n",
    "# os.makedirs(base_output_folder, exist_ok=True)\n",
    "\n",
    "# # Ensure the file exists\n",
    "# if not os.path.exists(input_file_path):\n",
    "#     print(f\"Error: The file '{input_file_path}' does not exist.\")\n",
    "# else:\n",
    "#     try:\n",
    "#         # Read the CSV file into a pandas DataFrame\n",
    "#         df = pd.read_csv(input_file_path)\n",
    "        \n",
    "#         # Strip any leading/trailing spaces in column names\n",
    "#         df.columns = df.columns.str.strip()\n",
    "\n",
    "#         # Check if the necessary columns exist\n",
    "#         required_columns = ['time_start', 'time_end', 'sender_id', 'receiver_id']\n",
    "#         missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        \n",
    "#         if missing_columns:\n",
    "#             print(f\"Error: The following required columns are missing: {', '.join(missing_columns)}\")\n",
    "#         else:\n",
    "#             # Create folder for matches if it doesn't exist\n",
    "#             match_folder = os.path.join(base_output_folder, 'matches')\n",
    "#             os.makedirs(match_folder, exist_ok=True)\n",
    "\n",
    "#             # Initialize variables\n",
    "#             current_match = 1\n",
    "#             match_data = []\n",
    "#             previous_end_time = None\n",
    "#             previous_receiver_id = None\n",
    "\n",
    "#             # Loop over the rows in the dataframe to create matches\n",
    "#             for i, row in df.iterrows():\n",
    "#                 time_start = row['time_start']\n",
    "#                 time_end = row['time_end']\n",
    "#                 sender_id = row['sender_id']\n",
    "#                 receiver_id = row['receiver_id']\n",
    "\n",
    "#                 # Debugging print to check each row\n",
    "#                 print(f\"Processing row {i}: time_start={time_start}, time_end={time_end}, sender_id={sender_id}, receiver_id={receiver_id}\")\n",
    "\n",
    "#                 # If this is the first row, create the first match\n",
    "#                 if i == 0:\n",
    "#                     match_data.append(row)\n",
    "#                     previous_end_time = time_end\n",
    "#                     previous_receiver_id = receiver_id\n",
    "#                     continue\n",
    "\n",
    "#                 # Check if the current time_start is within 3 minutes of previous time_end\n",
    "#                 time_diff = abs(pd.to_datetime(time_start) - pd.to_datetime(previous_end_time))\n",
    "\n",
    "#                 # Condition for matching based on time difference and sender-receiver IDs\n",
    "#                 if time_diff <= pd.Timedelta(minutes=3) and sender_id == previous_receiver_id:\n",
    "#                     match_data.append(row)\n",
    "#                     previous_end_time = time_end\n",
    "#                     previous_receiver_id = receiver_id\n",
    "#                 else:\n",
    "#                     # Save the current match to a file\n",
    "#                     if match_data:\n",
    "#                         match_filename = os.path.join(match_folder, f\"match_{current_match}.csv\")\n",
    "#                         match_df = pd.DataFrame(match_data)\n",
    "#                         match_df.to_csv(match_filename, index=False)\n",
    "#                         print(f\"Saved match {current_match} to {match_filename}\")\n",
    "                    \n",
    "#                     # Start a new match\n",
    "#                     current_match += 1\n",
    "#                     match_data = [row]\n",
    "#                     previous_end_time = time_end\n",
    "#                     previous_receiver_id = receiver_id\n",
    "\n",
    "#             # After loop, save the last match if any data remains\n",
    "#             if match_data:\n",
    "#                 match_filename = os.path.join(match_folder, f\"match_{current_match}.csv\")\n",
    "#                 match_df = pd.DataFrame(match_data)\n",
    "#                 match_df.to_csv(match_filename, index=False)\n",
    "#                 print(f\"Saved match {current_match} to {match_filename}\")\n",
    "    \n",
    "#     except PermissionError as e:\n",
    "#         print(f\"Permission error: {e}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seperating matches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "teams seperated on the basis of player id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # Define the path to your CSV file\n",
    "# input_file_path = 'passes.csv'  # Replace with your actual file path\n",
    "\n",
    "# # Define the base folder to save match files\n",
    "# base_output_folder = 'matches'\n",
    "\n",
    "# # Create the base output folder if it doesn't exist\n",
    "# os.makedirs(base_output_folder, exist_ok=True)\n",
    "\n",
    "# # List of the columns for match_a (x_1 to x_14)\n",
    "# columns_match_a = [\n",
    "#     'x_1', 'x_2', 'x_3', 'x_4', 'x_5', 'x_6', 'x_7', 'x_8', 'x_9', 'x_10', \n",
    "#     'x_11', 'x_12', 'x_13', 'x_14'\n",
    "# ]\n",
    "\n",
    "# # List of the columns for match_b (x_15 to x_28)\n",
    "# columns_match_b = [\n",
    "#     'x_15', 'x_16', 'x_17', 'x_18', 'x_19', 'x_20', 'x_21', 'x_22', 'x_23', 'x_24', \n",
    "#     'x_25', 'x_26', 'x_27', 'x_28'\n",
    "# ]\n",
    "\n",
    "# # Ensure that the file exists before proceeding\n",
    "# if not os.path.exists(input_file_path):\n",
    "#     print(f\"Error: The file '{input_file_path}' does not exist.\")\n",
    "# else:\n",
    "#     try:\n",
    "#         # Read the CSV file into a pandas DataFrame\n",
    "#         df = pd.read_csv(input_file_path)\n",
    "        \n",
    "#         # Check if the columns exist in the CSV file\n",
    "#         missing_columns_a = [col for col in columns_match_a if col not in df.columns]\n",
    "#         missing_columns_b = [col for col in columns_match_b if col not in df.columns]\n",
    "        \n",
    "#         if missing_columns_a:\n",
    "#             print(f\"Warning: The following columns are missing for match_a: {', '.join(missing_columns_a)}\")\n",
    "#         if missing_columns_b:\n",
    "#             print(f\"Warning: The following columns are missing for match_b: {', '.join(missing_columns_b)}\")\n",
    "        \n",
    "#         # Process match_a (columns x_1 to x_14)\n",
    "#         empty_pattern_a = df[columns_match_a].isna().astype(int).apply(lambda row: tuple(row), axis=1)\n",
    "#         grouped_a = df.groupby(empty_pattern_a)\n",
    "        \n",
    "#         # Create folder for match_a if it doesn't exist\n",
    "#         match_a_folder = os.path.join(base_output_folder, 'match_a')\n",
    "#         os.makedirs(match_a_folder, exist_ok=True)\n",
    "\n",
    "#         # Loop over each group and save to a separate CSV file for match_a\n",
    "#         file_counter_a = 1\n",
    "#         for pattern, group_data in grouped_a:\n",
    "#             # Create a new filename for match_a\n",
    "#             output_filename_a = os.path.join(match_a_folder, f\"match_a_{file_counter_a}.csv\")\n",
    "            \n",
    "#             # Save the group's data to a new CSV file inside match_a folder\n",
    "#             group_data.to_csv(output_filename_a, index=False)\n",
    "            \n",
    "#             # Print message to indicate the file has been saved\n",
    "#             print(f\"Saved group with pattern {pattern} to {output_filename_a}\")\n",
    "            \n",
    "#             # Increment the counter\n",
    "#             file_counter_a += 1\n",
    "        \n",
    "#         # Process match_b (columns x_15 to x_28)\n",
    "#         empty_pattern_b = df[columns_match_b].isna().astype(int).apply(lambda row: tuple(row), axis=1)\n",
    "#         grouped_b = df.groupby(empty_pattern_b)\n",
    "        \n",
    "#         # Create folder for match_b if it doesn't exist\n",
    "#         match_b_folder = os.path.join(base_output_folder, 'match_b')\n",
    "#         os.makedirs(match_b_folder, exist_ok=True)\n",
    "\n",
    "#         # Loop over each group and save to a separate CSV file for match_b\n",
    "#         file_counter_b = 1\n",
    "#         for pattern, group_data in grouped_b:\n",
    "#             # Create a new filename for match_b\n",
    "#             output_filename_b = os.path.join(match_b_folder, f\"match_b_{file_counter_b}.csv\")\n",
    "            \n",
    "#             # Save the group's data to a new CSV file inside match_b folder\n",
    "#             group_data.to_csv(output_filename_b, index=False)\n",
    "            \n",
    "#             # Print message to indicate the file has been saved\n",
    "#             print(f\"Saved group with pattern {pattern} to {output_filename_b}\")\n",
    "            \n",
    "#             # Increment the counter\n",
    "#             file_counter_b += 1\n",
    "        \n",
    "#     except PermissionError as e:\n",
    "#         print(f\"Permission error: {e}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "teams seperated on the basis of sender_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the path to your CSV file\n",
    "input_file_path = 'passes.csv'  # Replace with your actual file path\n",
    "\n",
    "# Define the base folder to save match files\n",
    "base_output_folder = 'matches'\n",
    "\n",
    "# Create the base output folder if it doesn't exist\n",
    "os.makedirs(base_output_folder, exist_ok=True)\n",
    "\n",
    "# List of the columns for match_a (x_1 to x_14)\n",
    "columns_match_a = [\n",
    "    'x_1', 'x_2', 'x_3', 'x_4', 'x_5', 'x_6', 'x_7', 'x_8', 'x_9', 'x_10', \n",
    "    'x_11', 'x_12', 'x_13', 'x_14'\n",
    "]\n",
    "\n",
    "# List of the columns for match_b (x_15 to x_28)\n",
    "columns_match_b = [\n",
    "    'x_15', 'x_16', 'x_17', 'x_18', 'x_19', 'x_20', 'x_21', 'x_22', 'x_23', 'x_24', \n",
    "    'x_25', 'x_26', 'x_27', 'x_28'\n",
    "]\n",
    "\n",
    "# Ensure that the file exists before proceeding\n",
    "if not os.path.exists(input_file_path):\n",
    "    print(f\"Error: The file '{input_file_path}' does not exist.\")\n",
    "else:\n",
    "    try:\n",
    "        # Read the CSV file into a pandas DataFrame\n",
    "        df = pd.read_csv(input_file_path)\n",
    "\n",
    "        # Separate the data based on sender_id\n",
    "        team_a_df = df[(df['sender_id'] >= 1) & (df['sender_id'] <= 14)]\n",
    "        team_b_df = df[(df['sender_id'] >= 15) & (df['sender_id'] <= 28)]\n",
    "\n",
    "        # Processing Team A\n",
    "        if not team_a_df.empty:\n",
    "            # Check if the columns exist in the team_a DataFrame\n",
    "            missing_columns_a = [col for col in columns_match_a if col not in team_a_df.columns]\n",
    "            if missing_columns_a:\n",
    "                print(f\"Warning: The following columns are missing for match_a: {', '.join(missing_columns_a)}\")\n",
    "            else:\n",
    "                empty_pattern_a = team_a_df[columns_match_a].isna().astype(int).apply(lambda row: tuple(row), axis=1)\n",
    "                grouped_a = team_a_df.groupby(empty_pattern_a)\n",
    "\n",
    "                # Create folder for match_a if it doesn't exist\n",
    "                match_a_folder = os.path.join(base_output_folder, 'match_a')\n",
    "                os.makedirs(match_a_folder, exist_ok=True)\n",
    "\n",
    "                # Save each group to a separate CSV file\n",
    "                for file_counter_a, (pattern, group_data) in enumerate(grouped_a, start=1):\n",
    "                    output_filename_a = os.path.join(match_a_folder, f\"match_a_{file_counter_a}.csv\")\n",
    "                    group_data.to_csv(output_filename_a, index=False)\n",
    "                    print(f\"Saved Team A group with pattern {pattern} to {output_filename_a}\")\n",
    "\n",
    "        # Processing Team B\n",
    "        if not team_b_df.empty:\n",
    "            # Check if the columns exist in the team_b DataFrame\n",
    "            missing_columns_b = [col for col in columns_match_b if col not in team_b_df.columns]\n",
    "            if missing_columns_b:\n",
    "                print(f\"Warning: The following columns are missing for match_b: {', '.join(missing_columns_b)}\")\n",
    "            else:\n",
    "                empty_pattern_b = team_b_df[columns_match_b].isna().astype(int).apply(lambda row: tuple(row), axis=1)\n",
    "                grouped_b = team_b_df.groupby(empty_pattern_b)\n",
    "\n",
    "                # Create folder for match_b if it doesn't exist\n",
    "                match_b_folder = os.path.join(base_output_folder, 'match_b')\n",
    "                os.makedirs(match_b_folder, exist_ok=True)\n",
    "\n",
    "                # Save each group to a separate CSV file\n",
    "                for file_counter_b, (pattern, group_data) in enumerate(grouped_b, start=1):\n",
    "                    output_filename_b = os.path.join(match_b_folder, f\"match_b_{file_counter_b}.csv\")\n",
    "                    group_data.to_csv(output_filename_b, index=False)\n",
    "                    print(f\"Saved Team B group with pattern {pattern} to {output_filename_b}\")\n",
    "\n",
    "    except PermissionError as e:\n",
    "        print(f\"Permission error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now doing more to lessen the matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# from datetime import timedelta\n",
    "\n",
    "# # Define the path to your CSV file\n",
    "# input_file_path = 'passes.csv'  # Replace with your actual file path\n",
    "\n",
    "# # Define the base folder to save match files\n",
    "# base_output_folder = 'matches'\n",
    "\n",
    "# # Create the base output folder if it doesn't exist\n",
    "# os.makedirs(base_output_folder, exist_ok=True)\n",
    "\n",
    "# # Define the time difference threshold in milliseconds\n",
    "# time_threshold_ms = 300_000  # 5 minutes\n",
    "\n",
    "# # Function to group data based on the conditions\n",
    "# def group_matches(df, columns, time_start_col, time_end_col):\n",
    "#     grouped_matches = []\n",
    "#     current_match = []\n",
    "#     last_row = None\n",
    "\n",
    "#     for index, row in df.iterrows():\n",
    "#         # Check for the empty containers (missing values)\n",
    "#         empty_pattern = tuple(row[columns].isna())\n",
    "\n",
    "#         # If there are no previous rows in the current group, initialize the match\n",
    "#         if last_row is None:\n",
    "#             current_match.append(row)\n",
    "#             last_row = row\n",
    "#             continue\n",
    "\n",
    "#         # Condition 1: Similar empty containers\n",
    "#         prev_empty_pattern = tuple(last_row[columns].isna())\n",
    "#         similar_empty_containers = prev_empty_pattern == empty_pattern\n",
    "\n",
    "#         # Condition 2: Time difference\n",
    "#         time_diff = row[time_start_col] - last_row[time_end_col]\n",
    "#         time_diff_exceeds_threshold = time_diff > time_threshold_ms\n",
    "\n",
    "#         # Check if we should create a new group\n",
    "#         if not similar_empty_containers or time_diff_exceeds_threshold:\n",
    "#             grouped_matches.append(pd.DataFrame(current_match))\n",
    "#             current_match = []\n",
    "\n",
    "#         # Add the current row to the group\n",
    "#         current_match.append(row)\n",
    "#         last_row = row\n",
    "\n",
    "#     # Add the last group if it's not empty\n",
    "#     if current_match:\n",
    "#         grouped_matches.append(pd.DataFrame(current_match))\n",
    "\n",
    "#     return grouped_matches\n",
    "\n",
    "# # Read the CSV file\n",
    "# df = pd.read_csv(input_file_path)\n",
    "\n",
    "# # Convert time columns to numeric for processing\n",
    "# df['time_start'] = pd.to_numeric(df['time_start'], errors='coerce')\n",
    "# df['time_end'] = pd.to_numeric(df['time_end'], errors='coerce')\n",
    "\n",
    "# # Separate data into Team A and Team B\n",
    "# team_a_df = df[(df['sender_id'] >= 1) & (df['sender_id'] <= 14)]\n",
    "# team_b_df = df[(df['sender_id'] >= 15) & (df['sender_id'] <= 28)]\n",
    "\n",
    "# # Columns to check for empty containers\n",
    "# columns_team_a = [f'x_{i}' for i in range(1, 15)]\n",
    "# columns_team_b = [f'x_{i}' for i in range(15, 29)]\n",
    "\n",
    "# # Process Team A and Team B matches\n",
    "# team_a_matches = group_matches(team_a_df, columns_team_a, 'time_start', 'time_end')\n",
    "# team_b_matches = group_matches(team_b_df, columns_team_b, 'time_start', 'time_end')\n",
    "\n",
    "# # Save matches to CSV files\n",
    "# def save_matches(matches, folder, team_name):\n",
    "#     team_folder = os.path.join(folder, team_name)\n",
    "#     os.makedirs(team_folder, exist_ok=True)\n",
    "\n",
    "#     for i, match in enumerate(matches, start=1):\n",
    "#         output_file = os.path.join(team_folder, f\"{team_name}_match_{i}.csv\")\n",
    "#         match.to_csv(output_file, index=False)\n",
    "#         print(f\"Saved {team_name} match to {output_file}\")\n",
    "\n",
    "# save_matches(team_a_matches, base_output_folder, 'team_a')\n",
    "# save_matches(team_b_matches, base_output_folder, 'team_b')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing invalid passes from csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def remove_unwanted_passes_from_csv(file_path):\n",
    "    \"\"\"\n",
    "    This function removes passes from players 1-14 to 15-28 (A to B)\n",
    "    and from players 15-28 to 1-14 (B to A) and updates the total pass count in the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Ensure the sender_id and receiver_id columns are present\n",
    "        if 'sender_id' not in df.columns or 'receiver_id' not in df.columns:\n",
    "            print(f\"Skipping file '{file_path}' - missing required columns.\")\n",
    "            return\n",
    "\n",
    "        # Remove passes between Team A (1-14) and Team B (15-28)\n",
    "        valid_passes = df[\n",
    "            ((df['sender_id'] >= 1) & (df['sender_id'] <= 14) & (df['receiver_id'] >= 1) & (df['receiver_id'] <= 14)) |\n",
    "            ((df['sender_id'] >= 15) & (df['sender_id'] <= 28) & (df['receiver_id'] >= 15) & (df['receiver_id'] <= 28))\n",
    "        ]\n",
    "\n",
    "        # Save the filtered DataFrame back to the file\n",
    "        valid_passes.to_csv(file_path, index=False)\n",
    "        print(f\"Updated '{file_path}' successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file '{file_path}': {e}\")\n",
    "\n",
    "\n",
    "# Define the base path for match folders\n",
    "base_path = \"matches\"\n",
    "\n",
    "# Iterate over each match folder (match_a and match_b)\n",
    "for match_folder in os.listdir(base_path):\n",
    "    folder_path = os.path.join(base_path, match_folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue  # Skip if it's not a directory\n",
    "\n",
    "    # Ensure the match folder is for either match_a or match_b\n",
    "    if match_folder.startswith(\"match_a\") or match_folder.startswith(\"match_b\"):\n",
    "        \n",
    "        # List all the .csv files inside the current folder\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if file.endswith('.csv'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "\n",
    "                    # Process the CSV file\n",
    "                    remove_unwanted_passes_from_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any row with any empty cell\n",
    "df2 = df2.dropna()\n",
    "# sorting based on first column (time)\n",
    "df2.sort_values(by=['time_start'], ascending=True)\n",
    "# saving as csv\n",
    "df2.to_csv('distances.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Making different patterns and target graphs\n",
    "    1. Making the csv into differnet snapshots of 10 seconds and saving them in different txt files in a pattern-graph folder\n",
    "    2. Making the csv into differnet snapshots of 1 minute and saving them in different txt files in a target-graph folder\n",
    "\n",
    "    file structure:\n",
    "    - pattern-graph\n",
    "        - pg1.txt\n",
    "        - pg2.txt\n",
    "        - pg3.txt\n",
    "        ...\n",
    "    - target-graph\n",
    "        - dg1.txt\n",
    "        - dg2.txt\n",
    "        - dg3.txt\n",
    "         ...\n",
    "\n",
    "         \n",
    "    Data in text file:\n",
    "        n ==> number of players\n",
    "        0 \"senderid\"\n",
    "        1 \"receiverid\"\n",
    "        2 \"senderid\"\n",
    "        ...\n",
    "        m ==> number of passes\n",
    "        sender receiver distance\n",
    "        sender receiver distance\n",
    "        ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pattern_Threshold = 0.1\n",
    "Target_Pattern_Threshold = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df2.copy()\n",
    "\n",
    "time_start = df['time_start'].min()\n",
    "time_end = df['time_end'].max()\n",
    "\n",
    "snapshots = []\n",
    "interval = Pattern_Threshold\n",
    "\n",
    "current_time = time_start\n",
    "while current_time <= time_end:\n",
    "    snapshot = df[(df['time_start'] >= current_time) & (df['time_start'] < current_time + interval)]\n",
    "    snapshots.append(snapshot)\n",
    "    current_time += interval\n",
    "\n",
    "interval = Target_Pattern_Threshold\n",
    "\n",
    "Target_snapshots = []\n",
    "current_time = time_start\n",
    "while current_time <= time_end:\n",
    "    snapshot = df[(df['time_start'] >= current_time) & (df['time_start'] < current_time + interval)]\n",
    "    Target_snapshots.append(snapshot)\n",
    "    current_time += interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# def Snapshot_Parser(snapshot):\n",
    "#     def find_index(unique_nodes, node):\n",
    "#         try:\n",
    "#             return unique_nodes.index(node)\n",
    "#         except ValueError:\n",
    "#             return -1\n",
    "\n",
    "#     # Reset the index to ensure we can iterate over the DataFrame correctly\n",
    "#     snapshot = snapshot.reset_index(drop=True)\n",
    "\n",
    "#     # Strip any leading/trailing spaces in column names\n",
    "#     snapshot.columns = snapshot.columns.str.strip()\n",
    "\n",
    "#     # Check column names for debugging\n",
    "#     print(\"Columns in snapshot:\", snapshot.columns)\n",
    "\n",
    "#     # Getting unique sender and receiver\n",
    "#     unique_sender = list(set(snapshot['sender_id']))  # Adjust 'sender_id' if necessary\n",
    "#     unique_receiver = list(set(snapshot['receiver_id']))  # Adjust 'receiver_id' if necessary\n",
    "\n",
    "#     # Merging the unique sender and receiver\n",
    "#     unique_nodes = unique_sender + unique_receiver\n",
    "#     unique_nodes = list(set(unique_nodes))\n",
    "\n",
    "#     n = len(unique_nodes)\n",
    "#     m = len(snapshot)\n",
    "\n",
    "#     # Making a list for passes (without the distance)\n",
    "#     Passes = []\n",
    "\n",
    "#     # Iterating through the snapshot\n",
    "#     for i in range(m):\n",
    "#         sender = snapshot.loc[i, 'sender_id']  # Adjust 'sender_id' if necessary\n",
    "#         receiver = snapshot.loc[i, 'receiver_id']  # Adjust 'receiver_id' if necessary\n",
    "        \n",
    "#         Passes.append([\n",
    "#             find_index(unique_nodes, sender), \n",
    "#             find_index(unique_nodes, receiver)\n",
    "#         ])\n",
    "\n",
    "#     return n, m, unique_nodes, Passes\n",
    "\n",
    "# # Directory to save the pattern graph files\n",
    "# path = 'Pattern-graph'\n",
    "\n",
    "# # Create the directory if it doesn't exist\n",
    "# if not os.path.exists(path):\n",
    "#     os.makedirs(path)\n",
    "\n",
    "# # Remove empty snapshots\n",
    "# snapshots = [s for s in snapshots if not s.empty]\n",
    "\n",
    "# # Iterate through each snapshot and create the corresponding text file\n",
    "# index = 1\n",
    "# for snapshot in snapshots:\n",
    "#     print(\"Processing snapshot\", index, \"out of\", len(snapshots))\n",
    "    \n",
    "#     # Get the unique nodes and passes (without distance)\n",
    "#     n, m, unique_nodes, Passes = Snapshot_Parser(snapshot)\n",
    "    \n",
    "#     # Create a text file for this snapshot\n",
    "#     file_path = os.path.join(path, f'pg{index}.txt')\n",
    "#     with open(file_path, 'w') as file:\n",
    "#         # Write the number of unique nodes\n",
    "#         file.write(f\"{n}\\n\")\n",
    "        \n",
    "#         # Write the unique nodes in the required format\n",
    "#         for value, node in enumerate(unique_nodes):\n",
    "#             file.write(f\"{value} \\\"{node}\\\"\\n\")\n",
    "        \n",
    "#         # Write the number of passes\n",
    "#         file.write(f\"{m}\\n\")\n",
    "        \n",
    "#         # Write the passes (sender, receiver) without distance\n",
    "#         for pass_ in Passes:\n",
    "#             file.write(f\"{pass_[0]} {pass_[1]}\\n\")\n",
    "        \n",
    "#         # Write the padding zeros\n",
    "#         for _ in range(m):\n",
    "#             file.write(\"0\\n\")\n",
    "    \n",
    "#     index += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def Snapshot_Parser(snapshot):\n",
    "    def find_index(unique_nodes, node):\n",
    "        try:\n",
    "            return unique_nodes.index(node)\n",
    "        except ValueError:\n",
    "            return -1\n",
    "\n",
    "    # Reset the index to ensure we can iterate over the DataFrame correctly\n",
    "    snapshot = snapshot.reset_index(drop=True)\n",
    "    \n",
    "    # Getting unique sender and receiver\n",
    "    unique_sender = list(set(snapshot['sender_id']))\n",
    "    unique_receiver = list(set(snapshot['receiver_id']))\n",
    "    \n",
    "    # Merging the unique sender and receiver\n",
    "    unique_nodes = unique_sender + unique_receiver\n",
    "    unique_nodes = list(set(unique_nodes))\n",
    "\n",
    "    n = len(unique_nodes)\n",
    "    m = len(snapshot)\n",
    "\n",
    "    # Making a list for passes\n",
    "    Passes = []\n",
    "\n",
    "    # Iterating through the snapshot\n",
    "    for i in range(m):\n",
    "        sender = snapshot.loc[i, 'sender_id']\n",
    "        receiver = snapshot.loc[i, 'receiver_id']\n",
    "        \n",
    "        Passes.append([find_index(unique_nodes, sender), find_index(unique_nodes, receiver)])\n",
    "\n",
    "    return n, m, unique_nodes, Passes\n",
    "\n",
    "def split_into_snapshots(data, interval_sec=30):\n",
    "    snapshot_size = interval_sec  # Number of rows per snapshot (assuming 1 row = 1 sec)\n",
    "    snapshots = [data[i:i + snapshot_size] for i in range(0, len(data), snapshot_size)]\n",
    "    return snapshots\n",
    "\n",
    "def passes_list_to_dict(Passes, unique_nodes):\n",
    "    passes_dict = {}\n",
    "    for pair in Passes:\n",
    "        key, value = pair[0], pair  # Use the first element as key and the pair as value\n",
    "        if key not in passes_dict:\n",
    "            passes_dict[key] = []\n",
    "        passes_dict[key].append(pair)\n",
    "\n",
    "    for ind in range(0, len(unique_nodes)):\n",
    "        if ind not in passes_dict.keys():\n",
    "            passes_dict[ind] = []\n",
    "\n",
    "    passes_dict = {key: passes_dict[key] for key in sorted(passes_dict)}\n",
    "    return passes_dict\n",
    "\n",
    "# Main processing\n",
    "matches_folder = \"matches\"\n",
    "output_base_path = \"Target-Graphs\"\n",
    "\n",
    "if not os.path.exists(matches_folder):\n",
    "    print(f\"Error: The folder '{matches_folder}' does not exist. Please create it and add match subfolders (matches_a, matches_b).\")\n",
    "else:\n",
    "    matches_a_folder = os.path.join(matches_folder, \"match_a\")\n",
    "    matches_b_folder = os.path.join(matches_folder, \"match_b\")\n",
    "    \n",
    "    if not os.path.exists(matches_a_folder) or not os.path.exists(matches_b_folder):\n",
    "        print(\"Error: One or both of the 'matches_a' or 'matches_b' folders do not exist.\")\n",
    "    else:\n",
    "        os.makedirs(output_base_path, exist_ok=True)\n",
    "        output_team_a_path = os.path.join(output_base_path, \"match_a\")\n",
    "        output_team_b_path = os.path.join(output_base_path, \"match_b\")\n",
    "        os.makedirs(output_team_a_path, exist_ok=True)\n",
    "        os.makedirs(output_team_b_path, exist_ok=True)\n",
    "\n",
    "        match_files_a = [f for f in os.listdir(matches_a_folder) if f.endswith('.csv')]\n",
    "        match_files_b = [f for f in os.listdir(matches_b_folder) if f.endswith('.csv')]\n",
    "\n",
    "        if not match_files_a and not match_files_b:\n",
    "            print(\"No match CSV files found in either 'matches_a' or 'matches_b' folders.\")\n",
    "        \n",
    "        team_a_counter, team_b_counter = 1, 1  # Counters for naming dg1, dg2, etc.\n",
    "\n",
    "        for match_file in match_files_a + match_files_b:\n",
    "            team_folder = 'match_a' if match_file in match_files_a else 'match_b'\n",
    "            match_file_path = os.path.join(matches_a_folder if team_folder == 'match_a' else matches_b_folder, match_file)\n",
    "\n",
    "            try:\n",
    "                match_data = pd.read_csv(match_file_path)\n",
    "                snapshots = split_into_snapshots(match_data)\n",
    "\n",
    "                if not snapshots or all(s.empty for s in snapshots):\n",
    "                    print(f\"Skipping {match_file}: No valid snapshots found.\")\n",
    "                    continue\n",
    "\n",
    "                for snapshot in snapshots:\n",
    "                    print(f\"Processing snapshot for match {match_file}...\")\n",
    "\n",
    "                    if team_folder == 'match_a':\n",
    "                        snapshot_a = snapshot[['sender_id', 'receiver_id'] + [col for col in snapshot.columns if col.startswith('x_') and int(col.split('_')[1]) <= 14]]\n",
    "                        n_a, m_a, unique_nodes_a, Passes_a = Snapshot_Parser(snapshot_a)\n",
    "                        Passes_a = passes_list_to_dict(Passes_a, unique_nodes_a)\n",
    "                        output_file_a = os.path.join(output_team_a_path, f'dg{team_a_counter}.txt')\n",
    "                        with open(output_file_a, 'w') as file:\n",
    "                            file.write(str(n_a) + '\\n')\n",
    "                            value = 0\n",
    "                            for node in unique_nodes_a:\n",
    "                                file.write(f\"{value} \\\"{node}\\\"\\n\")\n",
    "                                value += 1\n",
    "                            for pass_lists in Passes_a.values():\n",
    "                                file.write(f\"{len(pass_lists)}\\n\")\n",
    "                                for pass_ in pass_lists:\n",
    "                                    file.write(f\"{pass_[0]} {pass_[1]}\\n\")\n",
    "                        team_a_counter += 1\n",
    "\n",
    "                    if team_folder == 'match_b':\n",
    "                        snapshot_b = snapshot[['sender_id', 'receiver_id'] + [col for col in snapshot.columns if col.startswith('x_') and int(col.split('_')[1]) > 14]]\n",
    "                        n_b, m_b, unique_nodes_b, Passes_b = Snapshot_Parser(snapshot_b)\n",
    "                        Passes_b = passes_list_to_dict(Passes_b, unique_nodes_b)\n",
    "                        output_file_b = os.path.join(output_team_b_path, f'dg{team_b_counter}.txt')\n",
    "                        with open(output_file_b, 'w') as file:\n",
    "                            file.write(str(n_b) + '\\n')\n",
    "                            value = 0\n",
    "                            for node in unique_nodes_b:\n",
    "                                file.write(f\"{value} \\\"{node}\\\"\\n\")\n",
    "                                value += 1\n",
    "                            for pass_lists in Passes_b.values():\n",
    "                                file.write(f\"{len(pass_lists)}\\n\")\n",
    "                                for pass_ in pass_lists:\n",
    "                                    file.write(f\"{pass_[0]} {pass_[1]}\\n\")\n",
    "                        team_b_counter += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing match {match_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing snapshot for match match_a_1.csv...\n",
      "Processing snapshot for match match_a_1.csv...\n",
      "Processing snapshot for match match_a_10.csv...\n",
      "Processing snapshot for match match_a_10.csv...\n",
      "Processing snapshot for match match_a_10.csv...\n",
      "Processing snapshot for match match_a_10.csv...\n",
      "Processing snapshot for match match_a_10.csv...\n",
      "Processing snapshot for match match_a_10.csv...\n",
      "Processing snapshot for match match_a_10.csv...\n",
      "Processing snapshot for match match_a_10.csv...\n",
      "Processing snapshot for match match_a_10.csv...\n",
      "Processing snapshot for match match_a_10.csv...\n",
      "Processing snapshot for match match_a_10.csv...\n",
      "Processing snapshot for match match_a_10.csv...\n",
      "Processing snapshot for match match_a_11.csv...\n",
      "Processing snapshot for match match_a_11.csv...\n",
      "Processing snapshot for match match_a_11.csv...\n",
      "Processing snapshot for match match_a_12.csv...\n",
      "Processing snapshot for match match_a_12.csv...\n",
      "Processing snapshot for match match_a_12.csv...\n",
      "Processing snapshot for match match_a_12.csv...\n",
      "Processing snapshot for match match_a_12.csv...\n",
      "Processing snapshot for match match_a_12.csv...\n",
      "Processing snapshot for match match_a_12.csv...\n",
      "Processing snapshot for match match_a_12.csv...\n",
      "Processing snapshot for match match_a_12.csv...\n",
      "Processing snapshot for match match_a_12.csv...\n",
      "Processing snapshot for match match_a_12.csv...\n",
      "Processing snapshot for match match_a_12.csv...\n",
      "Processing snapshot for match match_a_12.csv...\n",
      "Processing snapshot for match match_a_12.csv...\n",
      "Processing snapshot for match match_a_12.csv...\n",
      "Processing snapshot for match match_a_12.csv...\n",
      "Processing snapshot for match match_a_12.csv...\n",
      "Processing snapshot for match match_a_12.csv...\n",
      "Processing snapshot for match match_a_13.csv...\n",
      "Processing snapshot for match match_a_13.csv...\n",
      "Processing snapshot for match match_a_14.csv...\n",
      "Processing snapshot for match match_a_14.csv...\n",
      "Processing snapshot for match match_a_14.csv...\n",
      "Processing snapshot for match match_a_14.csv...\n",
      "Processing snapshot for match match_a_15.csv...\n",
      "Processing snapshot for match match_a_15.csv...\n",
      "Processing snapshot for match match_a_15.csv...\n",
      "Processing snapshot for match match_a_15.csv...\n",
      "Processing snapshot for match match_a_15.csv...\n",
      "Processing snapshot for match match_a_15.csv...\n",
      "Processing snapshot for match match_a_15.csv...\n",
      "Processing snapshot for match match_a_15.csv...\n",
      "Processing snapshot for match match_a_16.csv...\n",
      "Processing snapshot for match match_a_16.csv...\n",
      "Processing snapshot for match match_a_16.csv...\n",
      "Processing snapshot for match match_a_16.csv...\n",
      "Processing snapshot for match match_a_16.csv...\n",
      "Processing snapshot for match match_a_16.csv...\n",
      "Processing snapshot for match match_a_16.csv...\n",
      "Processing snapshot for match match_a_16.csv...\n",
      "Processing snapshot for match match_a_17.csv...\n",
      "Processing snapshot for match match_a_18.csv...\n",
      "Processing snapshot for match match_a_19.csv...\n",
      "Processing snapshot for match match_a_19.csv...\n",
      "Processing snapshot for match match_a_19.csv...\n",
      "Processing snapshot for match match_a_2.csv...\n",
      "Processing snapshot for match match_a_2.csv...\n",
      "Processing snapshot for match match_a_20.csv...\n",
      "Processing snapshot for match match_a_20.csv...\n",
      "Processing snapshot for match match_a_21.csv...\n",
      "Processing snapshot for match match_a_22.csv...\n",
      "Processing snapshot for match match_a_22.csv...\n",
      "Processing snapshot for match match_a_23.csv...\n",
      "Processing snapshot for match match_a_24.csv...\n",
      "Processing snapshot for match match_a_24.csv...\n",
      "Processing snapshot for match match_a_24.csv...\n",
      "Processing snapshot for match match_a_25.csv...\n",
      "Processing snapshot for match match_a_25.csv...\n",
      "Processing snapshot for match match_a_25.csv...\n",
      "Processing snapshot for match match_a_25.csv...\n",
      "Processing snapshot for match match_a_25.csv...\n",
      "Processing snapshot for match match_a_25.csv...\n",
      "Processing snapshot for match match_a_25.csv...\n",
      "Processing snapshot for match match_a_25.csv...\n",
      "Processing snapshot for match match_a_25.csv...\n",
      "Processing snapshot for match match_a_25.csv...\n",
      "Processing snapshot for match match_a_25.csv...\n",
      "Processing snapshot for match match_a_25.csv...\n",
      "Processing snapshot for match match_a_25.csv...\n",
      "Processing snapshot for match match_a_26.csv...\n",
      "Processing snapshot for match match_a_26.csv...\n",
      "Processing snapshot for match match_a_26.csv...\n",
      "Processing snapshot for match match_a_27.csv...\n",
      "Processing snapshot for match match_a_27.csv...\n",
      "Processing snapshot for match match_a_28.csv...\n",
      "Processing snapshot for match match_a_28.csv...\n",
      "Processing snapshot for match match_a_29.csv...\n",
      "Processing snapshot for match match_a_3.csv...\n",
      "Processing snapshot for match match_a_3.csv...\n",
      "Processing snapshot for match match_a_3.csv...\n",
      "Processing snapshot for match match_a_3.csv...\n",
      "Processing snapshot for match match_a_3.csv...\n",
      "Processing snapshot for match match_a_3.csv...\n",
      "Processing snapshot for match match_a_3.csv...\n",
      "Processing snapshot for match match_a_3.csv...\n",
      "Processing snapshot for match match_a_3.csv...\n",
      "Processing snapshot for match match_a_3.csv...\n",
      "Processing snapshot for match match_a_3.csv...\n",
      "Processing snapshot for match match_a_3.csv...\n",
      "Processing snapshot for match match_a_30.csv...\n",
      "Processing snapshot for match match_a_31.csv...\n",
      "Processing snapshot for match match_a_31.csv...\n",
      "Processing snapshot for match match_a_31.csv...\n",
      "Processing snapshot for match match_a_31.csv...\n",
      "Processing snapshot for match match_a_31.csv...\n",
      "Processing snapshot for match match_a_31.csv...\n",
      "Processing snapshot for match match_a_31.csv...\n",
      "Processing snapshot for match match_a_31.csv...\n",
      "Processing snapshot for match match_a_31.csv...\n",
      "Processing snapshot for match match_a_31.csv...\n",
      "Processing snapshot for match match_a_31.csv...\n",
      "Processing snapshot for match match_a_31.csv...\n",
      "Processing snapshot for match match_a_31.csv...\n",
      "Processing snapshot for match match_a_32.csv...\n",
      "Processing snapshot for match match_a_32.csv...\n",
      "Processing snapshot for match match_a_33.csv...\n",
      "Processing snapshot for match match_a_34.csv...\n",
      "Processing snapshot for match match_a_35.csv...\n",
      "Processing snapshot for match match_a_36.csv...\n",
      "Processing snapshot for match match_a_37.csv...\n",
      "Processing snapshot for match match_a_38.csv...\n",
      "Processing snapshot for match match_a_38.csv...\n",
      "Processing snapshot for match match_a_38.csv...\n",
      "Processing snapshot for match match_a_38.csv...\n",
      "Processing snapshot for match match_a_39.csv...\n",
      "Processing snapshot for match match_a_39.csv...\n",
      "Processing snapshot for match match_a_4.csv...\n",
      "Processing snapshot for match match_a_4.csv...\n",
      "Processing snapshot for match match_a_4.csv...\n",
      "Processing snapshot for match match_a_4.csv...\n",
      "Processing snapshot for match match_a_4.csv...\n",
      "Processing snapshot for match match_a_4.csv...\n",
      "Processing snapshot for match match_a_4.csv...\n",
      "Processing snapshot for match match_a_4.csv...\n",
      "Processing snapshot for match match_a_4.csv...\n",
      "Processing snapshot for match match_a_40.csv...\n",
      "Processing snapshot for match match_a_40.csv...\n",
      "Processing snapshot for match match_a_40.csv...\n",
      "Processing snapshot for match match_a_40.csv...\n",
      "Processing snapshot for match match_a_41.csv...\n",
      "Processing snapshot for match match_a_41.csv...\n",
      "Processing snapshot for match match_a_42.csv...\n",
      "Skipping match_a_43.csv: No valid snapshots found.\n",
      "Processing snapshot for match match_a_44.csv...\n",
      "Processing snapshot for match match_a_44.csv...\n",
      "Processing snapshot for match match_a_45.csv...\n",
      "Processing snapshot for match match_a_45.csv...\n",
      "Processing snapshot for match match_a_46.csv...\n",
      "Processing snapshot for match match_a_47.csv...\n",
      "Processing snapshot for match match_a_47.csv...\n",
      "Processing snapshot for match match_a_47.csv...\n",
      "Processing snapshot for match match_a_47.csv...\n",
      "Processing snapshot for match match_a_47.csv...\n",
      "Processing snapshot for match match_a_47.csv...\n",
      "Processing snapshot for match match_a_47.csv...\n",
      "Processing snapshot for match match_a_47.csv...\n",
      "Processing snapshot for match match_a_47.csv...\n",
      "Processing snapshot for match match_a_5.csv...\n",
      "Processing snapshot for match match_a_5.csv...\n",
      "Processing snapshot for match match_a_5.csv...\n",
      "Processing snapshot for match match_a_5.csv...\n",
      "Processing snapshot for match match_a_5.csv...\n",
      "Processing snapshot for match match_a_5.csv...\n",
      "Processing snapshot for match match_a_5.csv...\n",
      "Processing snapshot for match match_a_6.csv...\n",
      "Processing snapshot for match match_a_6.csv...\n",
      "Processing snapshot for match match_a_6.csv...\n",
      "Processing snapshot for match match_a_6.csv...\n",
      "Processing snapshot for match match_a_6.csv...\n",
      "Processing snapshot for match match_a_6.csv...\n",
      "Processing snapshot for match match_a_6.csv...\n",
      "Processing snapshot for match match_a_7.csv...\n",
      "Processing snapshot for match match_a_7.csv...\n",
      "Processing snapshot for match match_a_7.csv...\n",
      "Processing snapshot for match match_a_7.csv...\n",
      "Processing snapshot for match match_a_8.csv...\n",
      "Processing snapshot for match match_a_8.csv...\n",
      "Processing snapshot for match match_a_8.csv...\n",
      "Processing snapshot for match match_a_8.csv...\n",
      "Processing snapshot for match match_a_8.csv...\n",
      "Processing snapshot for match match_a_8.csv...\n",
      "Processing snapshot for match match_a_8.csv...\n",
      "Processing snapshot for match match_a_8.csv...\n",
      "Processing snapshot for match match_a_8.csv...\n",
      "Processing snapshot for match match_a_8.csv...\n",
      "Processing snapshot for match match_a_8.csv...\n",
      "Processing snapshot for match match_a_8.csv...\n",
      "Processing snapshot for match match_a_8.csv...\n",
      "Processing snapshot for match match_a_8.csv...\n",
      "Processing snapshot for match match_a_8.csv...\n",
      "Processing snapshot for match match_a_8.csv...\n",
      "Processing snapshot for match match_a_8.csv...\n",
      "Processing snapshot for match match_a_8.csv...\n",
      "Processing snapshot for match match_a_8.csv...\n",
      "Processing snapshot for match match_a_8.csv...\n",
      "Processing snapshot for match match_a_8.csv...\n",
      "Processing snapshot for match match_a_9.csv...\n",
      "Processing snapshot for match match_a_9.csv...\n",
      "Processing snapshot for match match_a_9.csv...\n",
      "Processing snapshot for match match_b_1.csv...\n",
      "Processing snapshot for match match_b_1.csv...\n",
      "Processing snapshot for match match_b_10.csv...\n",
      "Processing snapshot for match match_b_10.csv...\n",
      "Processing snapshot for match match_b_10.csv...\n",
      "Processing snapshot for match match_b_10.csv...\n",
      "Processing snapshot for match match_b_10.csv...\n",
      "Processing snapshot for match match_b_10.csv...\n",
      "Processing snapshot for match match_b_10.csv...\n",
      "Processing snapshot for match match_b_10.csv...\n",
      "Processing snapshot for match match_b_10.csv...\n",
      "Processing snapshot for match match_b_10.csv...\n",
      "Processing snapshot for match match_b_10.csv...\n",
      "Processing snapshot for match match_b_11.csv...\n",
      "Processing snapshot for match match_b_11.csv...\n",
      "Processing snapshot for match match_b_12.csv...\n",
      "Processing snapshot for match match_b_12.csv...\n",
      "Processing snapshot for match match_b_12.csv...\n",
      "Processing snapshot for match match_b_13.csv...\n",
      "Processing snapshot for match match_b_13.csv...\n",
      "Processing snapshot for match match_b_14.csv...\n",
      "Processing snapshot for match match_b_14.csv...\n",
      "Processing snapshot for match match_b_14.csv...\n",
      "Processing snapshot for match match_b_14.csv...\n",
      "Processing snapshot for match match_b_14.csv...\n",
      "Processing snapshot for match match_b_14.csv...\n",
      "Processing snapshot for match match_b_14.csv...\n",
      "Processing snapshot for match match_b_14.csv...\n",
      "Processing snapshot for match match_b_14.csv...\n",
      "Processing snapshot for match match_b_15.csv...\n",
      "Processing snapshot for match match_b_15.csv...\n",
      "Processing snapshot for match match_b_15.csv...\n",
      "Processing snapshot for match match_b_15.csv...\n",
      "Processing snapshot for match match_b_16.csv...\n",
      "Processing snapshot for match match_b_16.csv...\n",
      "Processing snapshot for match match_b_16.csv...\n",
      "Processing snapshot for match match_b_16.csv...\n",
      "Processing snapshot for match match_b_16.csv...\n",
      "Processing snapshot for match match_b_16.csv...\n",
      "Processing snapshot for match match_b_16.csv...\n",
      "Processing snapshot for match match_b_16.csv...\n",
      "Processing snapshot for match match_b_16.csv...\n",
      "Processing snapshot for match match_b_16.csv...\n",
      "Processing snapshot for match match_b_16.csv...\n",
      "Processing snapshot for match match_b_17.csv...\n",
      "Processing snapshot for match match_b_17.csv...\n",
      "Processing snapshot for match match_b_18.csv...\n",
      "Processing snapshot for match match_b_18.csv...\n",
      "Processing snapshot for match match_b_19.csv...\n",
      "Processing snapshot for match match_b_19.csv...\n",
      "Processing snapshot for match match_b_2.csv...\n",
      "Processing snapshot for match match_b_2.csv...\n",
      "Processing snapshot for match match_b_2.csv...\n",
      "Processing snapshot for match match_b_2.csv...\n",
      "Processing snapshot for match match_b_2.csv...\n",
      "Processing snapshot for match match_b_2.csv...\n",
      "Processing snapshot for match match_b_2.csv...\n",
      "Processing snapshot for match match_b_20.csv...\n",
      "Processing snapshot for match match_b_20.csv...\n",
      "Processing snapshot for match match_b_21.csv...\n",
      "Processing snapshot for match match_b_22.csv...\n",
      "Processing snapshot for match match_b_23.csv...\n",
      "Processing snapshot for match match_b_24.csv...\n",
      "Processing snapshot for match match_b_25.csv...\n",
      "Processing snapshot for match match_b_25.csv...\n",
      "Processing snapshot for match match_b_25.csv...\n",
      "Processing snapshot for match match_b_25.csv...\n",
      "Processing snapshot for match match_b_25.csv...\n",
      "Processing snapshot for match match_b_25.csv...\n",
      "Processing snapshot for match match_b_26.csv...\n",
      "Processing snapshot for match match_b_26.csv...\n",
      "Processing snapshot for match match_b_27.csv...\n",
      "Processing snapshot for match match_b_28.csv...\n",
      "Processing snapshot for match match_b_29.csv...\n",
      "Processing snapshot for match match_b_29.csv...\n",
      "Processing snapshot for match match_b_3.csv...\n",
      "Processing snapshot for match match_b_3.csv...\n",
      "Processing snapshot for match match_b_3.csv...\n",
      "Processing snapshot for match match_b_3.csv...\n",
      "Processing snapshot for match match_b_3.csv...\n",
      "Processing snapshot for match match_b_3.csv...\n",
      "Processing snapshot for match match_b_3.csv...\n",
      "Processing snapshot for match match_b_30.csv...\n",
      "Processing snapshot for match match_b_30.csv...\n",
      "Processing snapshot for match match_b_30.csv...\n",
      "Processing snapshot for match match_b_31.csv...\n",
      "Processing snapshot for match match_b_31.csv...\n",
      "Processing snapshot for match match_b_31.csv...\n",
      "Processing snapshot for match match_b_31.csv...\n",
      "Processing snapshot for match match_b_31.csv...\n",
      "Processing snapshot for match match_b_31.csv...\n",
      "Processing snapshot for match match_b_31.csv...\n",
      "Processing snapshot for match match_b_32.csv...\n",
      "Processing snapshot for match match_b_32.csv...\n",
      "Processing snapshot for match match_b_32.csv...\n",
      "Processing snapshot for match match_b_32.csv...\n",
      "Processing snapshot for match match_b_32.csv...\n",
      "Processing snapshot for match match_b_32.csv...\n",
      "Processing snapshot for match match_b_33.csv...\n",
      "Processing snapshot for match match_b_33.csv...\n",
      "Processing snapshot for match match_b_34.csv...\n",
      "Processing snapshot for match match_b_34.csv...\n",
      "Processing snapshot for match match_b_34.csv...\n",
      "Processing snapshot for match match_b_34.csv...\n",
      "Processing snapshot for match match_b_35.csv...\n",
      "Processing snapshot for match match_b_35.csv...\n",
      "Processing snapshot for match match_b_35.csv...\n",
      "Processing snapshot for match match_b_35.csv...\n",
      "Processing snapshot for match match_b_35.csv...\n",
      "Processing snapshot for match match_b_36.csv...\n",
      "Processing snapshot for match match_b_36.csv...\n",
      "Processing snapshot for match match_b_37.csv...\n",
      "Processing snapshot for match match_b_37.csv...\n",
      "Processing snapshot for match match_b_38.csv...\n",
      "Processing snapshot for match match_b_39.csv...\n",
      "Processing snapshot for match match_b_39.csv...\n",
      "Processing snapshot for match match_b_39.csv...\n",
      "Processing snapshot for match match_b_39.csv...\n",
      "Processing snapshot for match match_b_39.csv...\n",
      "Processing snapshot for match match_b_39.csv...\n",
      "Processing snapshot for match match_b_39.csv...\n",
      "Processing snapshot for match match_b_39.csv...\n",
      "Processing snapshot for match match_b_39.csv...\n",
      "Processing snapshot for match match_b_39.csv...\n",
      "Processing snapshot for match match_b_4.csv...\n",
      "Processing snapshot for match match_b_4.csv...\n",
      "Processing snapshot for match match_b_4.csv...\n",
      "Processing snapshot for match match_b_4.csv...\n",
      "Processing snapshot for match match_b_4.csv...\n",
      "Processing snapshot for match match_b_4.csv...\n",
      "Processing snapshot for match match_b_40.csv...\n",
      "Processing snapshot for match match_b_40.csv...\n",
      "Processing snapshot for match match_b_40.csv...\n",
      "Processing snapshot for match match_b_40.csv...\n",
      "Processing snapshot for match match_b_41.csv...\n",
      "Processing snapshot for match match_b_41.csv...\n",
      "Processing snapshot for match match_b_41.csv...\n",
      "Processing snapshot for match match_b_41.csv...\n",
      "Processing snapshot for match match_b_42.csv...\n",
      "Processing snapshot for match match_b_42.csv...\n",
      "Processing snapshot for match match_b_42.csv...\n",
      "Processing snapshot for match match_b_42.csv...\n",
      "Processing snapshot for match match_b_42.csv...\n",
      "Processing snapshot for match match_b_42.csv...\n",
      "Processing snapshot for match match_b_42.csv...\n",
      "Processing snapshot for match match_b_42.csv...\n",
      "Processing snapshot for match match_b_42.csv...\n",
      "Processing snapshot for match match_b_42.csv...\n",
      "Processing snapshot for match match_b_42.csv...\n",
      "Processing snapshot for match match_b_43.csv...\n",
      "Processing snapshot for match match_b_43.csv...\n",
      "Processing snapshot for match match_b_44.csv...\n",
      "Processing snapshot for match match_b_45.csv...\n",
      "Processing snapshot for match match_b_46.csv...\n",
      "Processing snapshot for match match_b_47.csv...\n",
      "Processing snapshot for match match_b_47.csv...\n",
      "Processing snapshot for match match_b_48.csv...\n",
      "Processing snapshot for match match_b_48.csv...\n",
      "Processing snapshot for match match_b_49.csv...\n",
      "Processing snapshot for match match_b_5.csv...\n",
      "Processing snapshot for match match_b_5.csv...\n",
      "Processing snapshot for match match_b_5.csv...\n",
      "Processing snapshot for match match_b_5.csv...\n",
      "Processing snapshot for match match_b_5.csv...\n",
      "Processing snapshot for match match_b_5.csv...\n",
      "Processing snapshot for match match_b_5.csv...\n",
      "Processing snapshot for match match_b_5.csv...\n",
      "Processing snapshot for match match_b_5.csv...\n",
      "Processing snapshot for match match_b_5.csv...\n",
      "Processing snapshot for match match_b_5.csv...\n",
      "Processing snapshot for match match_b_5.csv...\n",
      "Processing snapshot for match match_b_5.csv...\n",
      "Processing snapshot for match match_b_50.csv...\n",
      "Processing snapshot for match match_b_6.csv...\n",
      "Processing snapshot for match match_b_6.csv...\n",
      "Processing snapshot for match match_b_6.csv...\n",
      "Processing snapshot for match match_b_7.csv...\n",
      "Processing snapshot for match match_b_8.csv...\n",
      "Processing snapshot for match match_b_8.csv...\n",
      "Processing snapshot for match match_b_8.csv...\n",
      "Processing snapshot for match match_b_9.csv...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def Snapshot_Parser(snapshot):\n",
    "    def find_index(unique_nodes, node):\n",
    "        try:\n",
    "            return unique_nodes.index(node)\n",
    "        except ValueError:\n",
    "            return -1\n",
    "\n",
    "    # Reset the index to ensure we can iterate over the DataFrame correctly\n",
    "    snapshot = snapshot.reset_index(drop=True)\n",
    "    \n",
    "    # Getting unique sender and receiver\n",
    "    unique_sender = list(set(snapshot['sender_id']))\n",
    "    unique_receiver = list(set(snapshot['receiver_id']))\n",
    "    \n",
    "    # Merging the unique sender and receiver\n",
    "    unique_nodes = list(set(unique_sender + unique_receiver))\n",
    "\n",
    "    n = len(unique_nodes)\n",
    "    m = len(snapshot)\n",
    "\n",
    "    # Making a list for passes\n",
    "    Passes = []\n",
    "\n",
    "    # Iterating through the snapshot\n",
    "    for i in range(m):\n",
    "        sender = snapshot.loc[i, 'sender_id']\n",
    "        receiver = snapshot.loc[i, 'receiver_id']\n",
    "        \n",
    "        Passes.append([find_index(unique_nodes, sender), find_index(unique_nodes, receiver)])\n",
    "\n",
    "    return n, m, unique_nodes, Passes\n",
    "\n",
    "def split_into_snapshots(data, interval_sec=30):\n",
    "    snapshot_size = interval_sec  # Number of rows per snapshot (assuming 1 row = 1 sec)\n",
    "    snapshots = [data[i:i + snapshot_size] for i in range(0, len(data), snapshot_size)]\n",
    "    return snapshots\n",
    "\n",
    "def passes_list_to_dict(Passes, unique_nodes):\n",
    "    passes_dict = {}\n",
    "    for pair in Passes:\n",
    "        key, value = pair[0], pair  # Use the first element as key and the pair as value\n",
    "        if key not in passes_dict:\n",
    "            passes_dict[key] = []\n",
    "        # Add the pair only if it is not already in the list\n",
    "        if pair not in passes_dict[key]:\n",
    "            passes_dict[key].append(pair)\n",
    "\n",
    "    # Ensure all indices have an entry in the dictionary\n",
    "    for ind in range(0, len(unique_nodes)):\n",
    "        if ind not in passes_dict.keys():\n",
    "            passes_dict[ind] = []\n",
    "\n",
    "    # Sort dictionary by keys\n",
    "    passes_dict = {key: passes_dict[key] for key in sorted(passes_dict)}\n",
    "    return passes_dict\n",
    "\n",
    "# Main processing\n",
    "matches_folder = \"matches\"\n",
    "output_base_path = \"Target-Graphs\"\n",
    "\n",
    "if not os.path.exists(matches_folder):\n",
    "    print(f\"Error: The folder '{matches_folder}' does not exist. Please create it and add match subfolders (matches_a, matches_b).\")\n",
    "else:\n",
    "    matches_a_folder = os.path.join(matches_folder, \"match_a\")\n",
    "    matches_b_folder = os.path.join(matches_folder, \"match_b\")\n",
    "    \n",
    "    if not os.path.exists(matches_a_folder) or not os.path.exists(matches_b_folder):\n",
    "        print(\"Error: One or both of the 'matches_a' or 'matches_b' folders do not exist.\")\n",
    "    else:\n",
    "        os.makedirs(output_base_path, exist_ok=True)\n",
    "        output_team_a_path = os.path.join(output_base_path, \"match_a\")\n",
    "        output_team_b_path = os.path.join(output_base_path, \"match_b\")\n",
    "        os.makedirs(output_team_a_path, exist_ok=True)\n",
    "        os.makedirs(output_team_b_path, exist_ok=True)\n",
    "\n",
    "        match_files_a = [f for f in os.listdir(matches_a_folder) if f.endswith('.csv')]\n",
    "        match_files_b = [f for f in os.listdir(matches_b_folder) if f.endswith('.csv')]\n",
    "\n",
    "        if not match_files_a and not match_files_b:\n",
    "            print(\"No match CSV files found in either 'matches_a' or 'matches_b' folders.\")\n",
    "        \n",
    "        team_a_counter, team_b_counter = 1, 1  # Counters for naming dg1, dg2, etc.\n",
    "\n",
    "        for match_file in match_files_a + match_files_b:\n",
    "            team_folder = 'match_a' if match_file in match_files_a else 'match_b'\n",
    "            match_file_path = os.path.join(matches_a_folder if team_folder == 'match_a' else matches_b_folder, match_file)\n",
    "\n",
    "            try:\n",
    "                match_data = pd.read_csv(match_file_path)\n",
    "                snapshots = split_into_snapshots(match_data)\n",
    "\n",
    "                if not snapshots or all(s.empty for s in snapshots):\n",
    "                    print(f\"Skipping {match_file}: No valid snapshots found.\")\n",
    "                    continue\n",
    "\n",
    "                for snapshot in snapshots:\n",
    "                    print(f\"Processing snapshot for match {match_file}...\")\n",
    "\n",
    "                    if team_folder == 'match_a':\n",
    "                        snapshot_a = snapshot[['sender_id', 'receiver_id'] + [col for col in snapshot.columns if col.startswith('x_') and int(col.split('_')[1]) <= 14]]\n",
    "                        n_a, m_a, unique_nodes_a, Passes_a = Snapshot_Parser(snapshot_a)\n",
    "                        Passes_a = passes_list_to_dict(Passes_a, unique_nodes_a)\n",
    "                        output_file_a = os.path.join(output_team_a_path, f'dg{team_a_counter}.txt')\n",
    "                        with open(output_file_a, 'w') as file:\n",
    "                            file.write(str(n_a) + '\\n')\n",
    "                            value = 0\n",
    "                            for node in unique_nodes_a:\n",
    "                                file.write(f\"{value} \\\"{node}\\\"\\n\")\n",
    "                                value += 1\n",
    "                            for pass_lists in Passes_a.values():\n",
    "                                file.write(f\"{len(pass_lists)}\\n\")\n",
    "                                for pass_ in pass_lists:\n",
    "                                    file.write(f\"{pass_[0]} {pass_[1]}\\n\")\n",
    "                        team_a_counter += 1\n",
    "\n",
    "                    if team_folder == 'match_b':\n",
    "                        snapshot_b = snapshot[['sender_id', 'receiver_id'] + [col for col in snapshot.columns if col.startswith('x_') and int(col.split('_')[1]) > 14]]\n",
    "                        n_b, m_b, unique_nodes_b, Passes_b = Snapshot_Parser(snapshot_b)\n",
    "                        Passes_b = passes_list_to_dict(Passes_b, unique_nodes_b)\n",
    "                        output_file_b = os.path.join(output_team_b_path, f'dg{team_b_counter}.txt')\n",
    "                        with open(output_file_b, 'w') as file:\n",
    "                            file.write(str(n_b) + '\\n')\n",
    "                            value = 0\n",
    "                            for node in unique_nodes_b:\n",
    "                                file.write(f\"{value} \\\"{node}\\\"\\n\")\n",
    "                                value += 1\n",
    "                            for pass_lists in Passes_b.values():\n",
    "                                file.write(f\"{len(pass_lists)}\\n\")\n",
    "                                for pass_ in pass_lists:\n",
    "                                    file.write(f\"{pass_[0]} {pass_[1]}\\n\")\n",
    "                        team_b_counter += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing match {match_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing unwanted passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Passes_a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for i in range(len(unique_nodes_a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_nodes_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def remove_unwanted_passes(file_content):\n",
    "#     \"\"\"\n",
    "#     This function removes passes from players 1-14 to 15-28 (A to B)\n",
    "#     and from players 15-28 to 1-14 (B to A) and updates the total pass count.\n",
    "#     \"\"\"\n",
    "#     # Parse the file content\n",
    "#     lines = file_content.strip().split(\"\\n\")\n",
    "    \n",
    "#     # First line is the number of passes\n",
    "#     total_passes = int(lines[0])\n",
    "    \n",
    "#     # Extract player node mappings (starting from the second line)\n",
    "#     node_mappings = []\n",
    "#     passes = []\n",
    "#     for i, line in enumerate(lines[1:], 1):\n",
    "#         if '\"' in line:\n",
    "#             node_mappings.append(line.strip())  # Player node mappings\n",
    "#         else:  # The passes start here\n",
    "#             pass_data = line.strip().split()\n",
    "#             if len(pass_data) == 2:\n",
    "#                 passes.append([int(pass_data[0]), int(pass_data[1])])\n",
    "\n",
    "#     # Remove unwanted passes from player 1-14 to 15-28 (A to B) and 15-28 to 1-14 (B to A)\n",
    "#     valid_passes = []\n",
    "#     for p in passes:\n",
    "#         sender, receiver = p\n",
    "#         # Check if both sender and receiver are in the same range (1-14 to 1-14, or 15-28 to 15-28)\n",
    "#         if (0 <= sender <= 13 and 0 <= receiver <= 13) or (14 <= sender <= 27 and 14 <= receiver <= 27):\n",
    "#             valid_passes.append(p)\n",
    "#         else:\n",
    "#             total_passes -= 1  # Decrement the total pass count\n",
    "\n",
    "#     # Now we will reconstruct the file content\n",
    "#     result = []\n",
    "#     result.append(str(total_passes))  # Updated pass count\n",
    "#     result.extend(node_mappings)  # Player nodes\n",
    "#     for pass_ in valid_passes:\n",
    "#         result.append(f\"{pass_[0]} {pass_[1]}\")  # Valid passes\n",
    "#     result.extend(['0'] * total_passes)  # Zeros for the remaining part\n",
    "    \n",
    "#     return \"\\n\".join(result)\n",
    "\n",
    "# # Process all the .txt files in the Pattern-graphs folder\n",
    "# base_path = \"Pattern-graphs\"  # Base folder for the pattern directories\n",
    "\n",
    "# # Iterate over each match folder (match_a_number and match_b_number)\n",
    "# for match_folder in os.listdir(base_path):\n",
    "#     folder_path = os.path.join(base_path, match_folder)  # Path to the match folder\n",
    "#     if not os.path.isdir(folder_path):\n",
    "#         continue  # Skip if it's not a directory\n",
    "    \n",
    "#     # Ensure the match folder is for either match_a or match_b\n",
    "#     if match_folder.startswith(\"match_a\") or match_folder.startswith(\"match_b\"):\n",
    "        \n",
    "#         # List all the txt files inside the current folder\n",
    "#         txt_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "        \n",
    "#         if not txt_files:\n",
    "#             print(f\"No txt files found in folder '{folder_path}'.\")\n",
    "#             continue\n",
    "        \n",
    "#         # Process each .txt file\n",
    "#         for txt_file in txt_files:\n",
    "#             txt_file_path = os.path.join(folder_path, txt_file)\n",
    "            \n",
    "#             try:\n",
    "#                 # Read the content of the txt file\n",
    "#                 with open(txt_file_path, 'r') as file:\n",
    "#                     file_content = file.read()\n",
    "\n",
    "#                 # Remove unwanted passes and update the content\n",
    "#                 updated_content = remove_unwanted_passes(file_content)\n",
    "\n",
    "#                 # Write the updated content back to the same txt file\n",
    "#                 with open(txt_file_path, 'w') as file:\n",
    "#                     file.write(updated_content)\n",
    "                \n",
    "#                 print(f\"Updated {txt_file} successfully.\")\n",
    "            \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing {txt_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating PG folder dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# def Snapshot_Parser(snapshot):\n",
    "#     def find_index(unique_nodes, node):\n",
    "#         for i in range(len(unique_nodes)):\n",
    "#             if unique_nodes[i] == node:\n",
    "#                 return i-1\n",
    "#         return -1\n",
    "#     # getting unique sender and receiver\n",
    "#     unique_sender = list(set(snapshot['Sender']))\n",
    "#     unique_receiver = list(set(snapshot['Receiver']))  \n",
    "#     # merging the unique sender and receiver\n",
    "#     unique_nodes = unique_sender + unique_receiver\n",
    "#     unique_nodes = list(set(unique_nodes))\n",
    "#     n = len(unique_nodes)\n",
    "#     m = len(snapshot)\n",
    "#     # making a 2d list for passes\n",
    "#     passes = []\n",
    "#     # iterating through the snapshot\n",
    "#     Passes = []\n",
    "#     for i in range(m):\n",
    "#         # print(find_index(unique_nodes,snapshot['Sender'][i]), find_index(unique_nodes,snapshot['Receiver'][i]), snapshot['Distance'][i])\n",
    "#         Passes.append([find_index(unique_nodes,snapshot['Sender'][i]), find_index(unique_nodes,snapshot['Receiver'][i]), int(snapshot['Distance'][i])])\n",
    "#     return n,m,unique_nodes,Passes\n",
    "# '''\n",
    "# def Snapshot_Parser(snapshot):\n",
    "#     def find_index(unique_nodes, node):\n",
    "#         try:\n",
    "#             return unique_nodes.index(node)\n",
    "#         except ValueError:\n",
    "#             return -1\n",
    "\n",
    "#     # Reset the index to ensure we can iterate over the DataFrame correctly\n",
    "#     snapshot = snapshot.reset_index(drop=True)\n",
    "    \n",
    "#     # getting unique sender and receiver\n",
    "#     unique_sender = list(set(snapshot['Sender']))\n",
    "#     unique_receiver = list(set(snapshot['Receiver']))\n",
    "    \n",
    "#     # merging the unique sender and receiver\n",
    "#     unique_nodes = unique_sender + unique_receiver\n",
    "#     unique_nodes = list(set(unique_nodes))\n",
    "\n",
    "#     n = len(unique_nodes)\n",
    "#     m = len(snapshot)\n",
    "\n",
    "#     # making a list for passes\n",
    "#     Passes = []\n",
    "\n",
    "#     # iterating through the snapshot\n",
    "#     for i in range(m):\n",
    "#         sender = snapshot.loc[i, 'Sender']\n",
    "#         receiver = snapshot.loc[i, 'Receiver']\n",
    "#         distance = snapshot.loc[i, 'Distance']\n",
    "        \n",
    "#         Passes.append([\n",
    "#             find_index(unique_nodes, sender), \n",
    "#             find_index(unique_nodes, receiver), \n",
    "#             int(distance)\n",
    "#         ])\n",
    "\n",
    "#     return n, m, unique_nodes, Passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'Pattern-graph'\n",
    "\n",
    "# # making directory name path\n",
    "# import os\n",
    "# if not os.path.exists(path):\n",
    "#     os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # removing all empty snapshots\n",
    "# snapshots = [s for s in snapshots if not s.empty]\n",
    "# '''\n",
    "# # Process each snapshot\n",
    "# for s in snapshots:\n",
    "#     try:\n",
    "#         result = Snapshot_Parser(s)\n",
    "#         print(result)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing snapshot: {e}\")\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 1\n",
    "# for snapshot in snapshots:\n",
    "#     print(\"Processing snapshot\", index , \"out of \", len(snapshots))\n",
    "#     n,m,unique_nodes,Passes = Snapshot_Parser(snapshot)\n",
    "#     # making a txt file in directory\n",
    "#     file = open(path + '/pg' + str(index) + '.txt','w')\n",
    "#     file.write(str(n) + '\\n')\n",
    "#     # writing the unique nodes in the txt file\n",
    "#     value = 0 \n",
    "#     for node in unique_nodes:\n",
    "#         file.write(str(value) + ' ' +\"\\\"\"+str(node)+\"\\\"\" + '\\n')\n",
    "#         value += 1\n",
    "#     file.write(str(m) + '\\n')\n",
    "#     # writing the passes in the txt file\n",
    "#     for pass_ in Passes:\n",
    "#         file.write(str(pass_[0]) + ' ' + str(pass_[1]) + ' ' + str(int(pass_[2])) + '\\n')\n",
    "    \n",
    "#     for i in range(m):\n",
    "#         file.write(\"0\" + '\\n')\n",
    "\n",
    "#     file.close()\n",
    "#     index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Target folder dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # removing all empty snapshots\n",
    "# Target_snapshots = [s for s in Target_snapshots if not s.empty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'target-graph'\n",
    "\n",
    "# # making directory name path\n",
    "# import os\n",
    "# if not os.path.exists(path):\n",
    "#     os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 1\n",
    "# for snapshot in Target_snapshots:\n",
    "#     print(\"Processing snapshot\", index , \"out of \", len(Target_snapshots))\n",
    "#     n,m,unique_nodes,Passes = Snapshot_Parser(snapshot)\n",
    "#     # making a txt file in directory\n",
    "#     file = open(path + '/dg' + str(index) + '.txt','w')\n",
    "#     file.write(str(n) + '\\n')\n",
    "#     # writing the unique nodes in the txt file\n",
    "#     value = 0\n",
    "#     for node in unique_nodes:\n",
    "#         file.write(str(value) + ' ' +\"\\\"\"+str(node)+\"\\\"\" + '\\n')\n",
    "#         value += 1\n",
    "\n",
    "#     # finding snapshot within the target snapshot\n",
    "#     Subsnap = []\n",
    "#     interval = Pattern_Threshold\n",
    "\n",
    "#     current_time = time_start\n",
    "#     while current_time <= time_end:\n",
    "#         snapshot = df[(df['time_start'] >= current_time) & (df['time_start'] < current_time + interval)]\n",
    "#         Subsnap.append(snapshot)\n",
    "#         current_time += interval\n",
    "    \n",
    "#     for i in range(len(Subsnap)):\n",
    "#         n,m,unique_nodes,Passes = Snapshot_Parser(Subsnap[i])\n",
    "#         if m != 0:\n",
    "#             file.write(str(m) + '\\n')\n",
    "#             # writing the passes in the txt file\n",
    "#             for pass_ in Passes:\n",
    "#                 file.write(str(pass_[0]) + ' ' + str(pass_[1]) + ' ' + str(int(pass_[2])) + '\\n')\n",
    "#         else:\n",
    "#             file.write(\"0\" + '\\n')\n",
    "\n",
    "#     file.close()\n",
    "#     index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
